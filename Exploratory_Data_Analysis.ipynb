{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "It is an approach to analyze data in order to:\n",
    "1. summarize main charateristics of the data to gain better understanding of the dataset.\n",
    "2. uncover relationships between different variables\n",
    "3. extract important variables for the problems we are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Descriptive Statistics\n",
    "* Describe basic features of data\n",
    "* Giving short summaries about the sample and measures of the data.\n",
    "* One way in which we can do this is by using the `describe()` method in pandas.\n",
    "* Using the `describe()` function and applying it, automatically computes basic statistics for all numerical variables.\n",
    "* Any NaN vaules are automatically skipped in these statistics.\n",
    "* for categorical values, one way we can summarize it is by using the function `value_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_wheels_counts = df['drive-wheels'].value_counts()\n",
    "drive_wheels_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Box plot` are a great way to visualize numeric data since we can visualize the various distribution of the data.\n",
    "* The main feature that the box plot shows are: \n",
    "    1. The `median` of the data, which represents where the middle datapoint is.\n",
    "    2. The `upper-quartile` shows where the 75th percentile is.\n",
    "    3. The `lower quartile` shows where the 25th percentile is.\n",
    "    4. Next we have the `lower-extremes` and `upper-extremes`. These are calculated as 1.5 times the inter-quartile range (IQR) above and below `75th` and `25th` percentile respectively.\n",
    "    5. Finally `box plots` also display `outliers` as individulal dots that occurs outside the upper and lower extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x = 'drive-wheels', y = 'price', data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of price between the rwd (rear-wheel drive) and fwd (forward-wheel drive) and othe categories are distinct, but the price for fwd (forward-wheel drive) are almost indistinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Often times we tend to see continuous variables in our data. These data points are numbers contained in some range. \n",
    "* For example: \n",
    "    1. In out dataset, price and enigne-size are continuous variables. What if we want to know what the distribution the relationship between these two columns?\n",
    "    2. One way to visualize this is by using `scatter plots`.\n",
    "    3. Each observation in a scatter plot is represented as a point. \n",
    "    4. This plot shows the relationship between two variables:\n",
    "        * The `predictor variable` is plotted on the `x-axis`.\n",
    "        * The `target variable` is plotted on the `y-axis`.\n",
    "    5. In a scatter plot, we typically set the predictor variable on the x-axis or horizontal axis and we set the target variable on the y-axis or vertical axis.\n",
    "    * We are using the Matplotlib functioon `scatter()` using taking in x and a y variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(df['engine-size'], df['price'], color='red')\n",
    "plt.title('Scatterplot of Engine Size vs Price') # Add a title to the scatterplot\n",
    "plt.xlabel('Engine Size') # Add x-label to the scatterplot\n",
    "plt.ylabel('Price') # Add y-label to the scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group By Data In Python\n",
    "## Grouping Data\n",
    "* Assume we want to know:\n",
    "    1. Is there any relationship between tehe differnt types of `drive-system`.\n",
    "    2. If so, which type of `drive-system` adds the most value to a vehicle.\n",
    "\n",
    "* It would be nice if we could group all data by the different types of drive wheels, and compare the results of these different drive wheels against each other.\n",
    "* In pandas this can be done using `group-by()` method.\n",
    "* The `group-by()` method is used on categorical variables, groups the data into subset according to the different categories of that variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[['drive-wheels', 'body-style', 'price']] #pick out the values we are interested in\n",
    "df_grp = df_test.groupby(['drive-wheels', 'body-style'], as_index=False).mean() #we group the reduced data accroding to the 'drive-wheel' and'body-style'\n",
    "#Since we are interested in knowing, how the average price differs accross he boards, we can take the maen of each group and append this bit at the very ed of the above line.\n",
    "df_grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that according to our data, `rear-wheel-drive` `convertible` and `rear-wheel-drive` `hardtop` have the highest value, while the `four-wheel-drive` `hatchbacks` have the lowest.\n",
    "* A table of ths form isn't the easiest to read, and also not very easy to visualize. \n",
    "* To make it easier to understand, we can convert this table into a `pivot table` using `pivot()` method.\n",
    "* A `pivot table` is like an Excel spreadsheet, with one variable along the column and another along the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_grp.pivot(index='drive-wheels', columns='body-style') #we can convert the above table into a pivot table\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The price data now becoms the rectangular grid which is easier to understand.\n",
    "* Another way to represent the `pivot table` is by using `heat map` which puts different colors in the cells to represent the value.\n",
    "* `HeatMap` takes a rectangular grid of data and assigns a color intensity based on the data value at the grid points.\n",
    "* It is a great way to plot the target variable over multiple variables and through this get visual clues of the relationship between these variables and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(df_pivot, cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Variance (ANOVA)\n",
    "* Assume, that we want to anlysze a categorical variable and see the correlation among different categories. \n",
    "* To analyze categorical variabele such the `make` variable, we can use a method such as `ANOVA` method.\n",
    "* `ANOVA` is a statistical method used to test whether there are significant differences between the means of two or more groups.\n",
    "* `ANOVA` can be used to find the correlation between different groups of a categorical variable.\n",
    "* The `ANOVA` test returns two values:\n",
    "    1. `F-test score`: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.\n",
    "    2. `P-value`: P-value tells how statistically significant is our calculated score value.\n",
    "* To implement `ANOVA` test:\n",
    "    1. First we extract the `make` and `price` data.\n",
    "    2. Then we will group the data by different makes.\n",
    "    3. The `ANOVA` test can be performed in python usig the `f_oneway()` function in the `stats` module in the `scipy` library.\n",
    "    4. The `f_oneway()` function takes the groups we want to compare as input and returns `F-test score` and `P-value`.\n",
    "    5. We then pass in the data of the two car make groups we want to compare and it calculates the `ANOVA` results.\n",
    "    5. If the `P-value` is less than a certain `significance level` (in this case 0.05), we can reject the null hypothesis and say that the `alternative hypothesis` is true.\n",
    "    6. If the `P-value` is larger than the `significance level`, we can accept the null hypothesis and say that `null hypothesis` is true.\n",
    "    7. If we get a large `F-test score` and a small `P-value`, we can reject the null hypothesis and say that the `alternative hypothesis` is true.\n",
    "    8. If we get a small `F-test score` and a large `P-value`, we cannot reject the null hypothesis and cannot say that the `alternative hypothesis` is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "df_anova = df[['make', 'price']]\n",
    "grouped_anova = df_anova.groupby(['make'])\n",
    "anova_results_1 = stats.f_oneway(grouped_anova.get_group('honda')['price'], grouped_anova.get_group('subaru')['price'])\n",
    "anova_results_2 = stats.f_oneway(grouped_anova.get_group('honda')['price'], grouped_anova.get_group('jaguar')['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Anova result: {anova_results_1}\")\n",
    "print(f\"Anova Result: {anova_results_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results confirm what we guessed at first. The price between `honda` and `subaru` is not significantly different, while the price between `jaguar` and `honda` or `jaguar` and `subaru` is significantly different.\n",
    "* All in all, we can say that there is a strong correlation between a categorical variable and other variables if `ANOVA` test gives us a large `F-test` value and a small `P-value`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "* `Correation` is a statistical metric for measuring to what extent different variables are interdependent.\n",
    "* It is important to know that `correlation` doesn't imply `causation`.\n",
    "* In other words, `Correlation` is a measure of how two variables change together, but it doesn't tell us about the cause and effect relationship between the two variables. \n",
    "* To see the coorelation we will visulaize it using `scatter plot` and add a linear line called a `regression line` between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='engine-size', y='price', data=df)\n",
    "plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the straight line through the data points is very steep, which shows that there is a strong positive correlation between `engine-size` and `price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='highway-mpg', y='price', data=df)\n",
    "plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cn see in this plot, when `highway-mpg` goes up, the `price` goes down, and when `highway-mpg` goes down, the `price` goes up. This indicates an `inverse/negative relationship` between these two variables. `Highway-mpg` could potentially be a predictor of `price`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation - Statistics\n",
    "* One way to measure the strength of the correlation between continuous numeric variable is by using a method called `Pearson Correlation`.\n",
    "* The `Pearson Correlation` method will give you two values: \n",
    "    1. `Correlation Coefficient`: For the coorelation coefficient, a value close to 1 implies a large positive correlation, a value close to -1 implies a large negative correlation, and a value close to 0 implies no correlation between the two variables.\n",
    "    2. `P-value`: The P-value tells us how certain we are about the correlation between the two variables. Generally, a `P-value less than 0.001` gives us `strong certainty` about the correlation coefficient that we calculated, a value `between 0.001 and 0.05` gives us `moderate certainty`, a `P-value between 0.05 and 0.1` gives us `weak certainty` and a `P-value larger than 0.1` will give us `no certainty` about the correlation coefficient that we calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['horsepower'] = df['horsepower'].astype(float)\n",
    "mean_hp = df['horsepower'].mean()\n",
    "df['horsepower'].replace(np.nan, mean_hp, inplace=True)\n",
    "df['horsepower'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['price'], axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pears_coef, p_value = stats.pearsonr(df['horsepower'], df['price'])\n",
    "pears_coef, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the `pearson-coefficient` is close to `1` hence their is a strong positive correlation between `horsepower` and `price` the `P-value` is `<0.001`, this means that we are `certain` about the `positive correlation` between `horsepower` and `price`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Heatmap\n",
    "* A `correlation heatmap` is a `heatmap` that shows the `correlation matrix`.\n",
    "* Taking all variables into account, we can calculate the `correlation` between every pair of variables, `correlation` between pairs of variables is calculated using three methods:\n",
    "    1. `Pearson`: Standard correlation coefficient\n",
    "    2. `Kendall`: Kendall Tau correlation coefficient\n",
    "    3. `Spearman`: Spearman rank correlation coefficient\n",
    "* Each of these methods have their own strengths and weaknesses, and can be used in different scenarios.\n",
    "* To build a `correlation heatmap`, we first need to calculate the `correlation matrix` that contains the `correlation` between each pair of variables.\n",
    "* We can then plot the `correlation matrix` using a `heatmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let us check about the missing values in categorical columns\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some missing values in categorical columns. We have to handle these columns.\n",
    "* As we had learnied that we can change the data type of the columns. By this is only possible if data filled is in other data-type as in case of `normalized-losses`, `bore`, `stroke`, `horsepower`, `peak-rpm` and `price` columns.\n",
    "* But in case of `make` column, we have to replace the missing values with the most frequent value in the column.\n",
    "* We can use `value_counts()` method to find the most frequent value in the column.\n",
    "* We can use `replace()` method to replace the missing values with the most frequent value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first list of columns with pure object type values\n",
    "cat_cols = ['make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'engine-type', 'num-of-cylinders', 'fuel-system', 'price-binned']\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a list of columns with numerical type values\n",
    "num_cols = ['symboling', 'normalized-losses', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-size', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-L/100Km', 'highway-mpg', 'price']\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, first we have to fill missing data in categorical columns. For this we have `SimpleImputer` class in `sklearn.impute` module.\n",
    "* `SimpleImputer` class provides basic strategies for imputing missing values, either using the `mean`, `median` or `most frequent` value of the column in which the missing values are located.\n",
    "* We can use `SimpleImputer` class to replace the missing values with the most frequent value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer.fit(df[cat_cols])\n",
    "df[cat_cols] = imputer.transform(df[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if there are any missing values in the categorical columns\n",
    "def fill_na(col):\n",
    "    df[col] = df[col].astype(float)\n",
    "    mean = df[col].mean()\n",
    "    df[col].replace(np.nan, mean, inplace=True)\n",
    "    return df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    col = fill_na(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now for plotting heatmap chart first we have to encode all the catagorical columns into numerical data. \n",
    "* For this we can use `encoders` from `sklearn` library.\n",
    "* There are basically three types of encoders:\n",
    "    1. `Label Encoder`: It is used to encode the catagorical data into numerical data when .\n",
    "    2. `One Hot Encoder`: It is used to encode the catagorical data into numerical data and create dummy variables for each catagory.\n",
    "    3. `Ordinal Encoders`: It is used to encode the catagorical data into numerical data when the catagories have some order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    df[col] = encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to generate heatmap\n",
    "plt.figure(figsize=(25,15))\n",
    "matrix = np.triu(df.corr())\n",
    "sns.heatmap(df.corr(), annot=True, mask=matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
