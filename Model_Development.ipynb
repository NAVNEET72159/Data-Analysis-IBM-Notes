{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "- A model or estimator can be thought of as a mathematical equation used to predict a value given one or more other values.\n",
    "- Relating one or more independent variables to a dependent variable.\n",
    "- Usually the more relevant data we have the more accurate our model is.\n",
    "- To understand why more data is important, consider the following situation:\n",
    "    * We have two almost identical cars.\n",
    "    * We want to use our mode to determine the price of two cars, one is pink car and other is red.\n",
    "    * If our model's independent variable or feature does not include color, our model will predict the same price for both cars.\n",
    "    * If our model's independent variable or feature does include color, our model will predict a higher price for the red car than the pink car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liner Regression and Multiple Regression\n",
    "* `Linear Regression` wil refer ti one independent variable to make a prediction.\n",
    "* `Multiple Regression` will refer to two or more independent variables to make a prediction.\n",
    "* `Simple Linear Regression (SLR)` is a method to help us understand the relationship between two variables:\n",
    "    * The predictor/independent variable (X)\n",
    "    * The response/dependent variable (that we want to predict)(Y)\n",
    "* The result of Linear Regression is a `linear function` that predicts the response (dependent) variable as a function of the predictor (independent) variable.\n",
    "* `Y: Response/Target/Dependent Variable`\n",
    "* `X: Predictor/Independent Variable`\n",
    "* `Y = b0 + b1X`\n",
    "* `b0: Intercept`\n",
    "* `b1: Slope`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, many factors influence how much people pay for a car, for example, how old the car is?\n",
    "- This uncertainty is represented by the error term, also called the `noise`.\n",
    "- The `noise` is the part of the data that can not be explained by the model, because there are other variables that influence the price that have not been included in the model.\n",
    "- The `noise` is the difference between the true value of the dependent variable, and the predicted value of the dependent variable.\n",
    "- It can be removed by using `residual plot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation using visualization\n",
    "* `Regression plot` are good extimate of:\n",
    "    - The relationship between two variable.\n",
    "    - The strength of correlation and \n",
    "    - The direction of the relationship.\n",
    "\n",
    "* The horizontal axis is the independent variable and the vertical axis is the dependent variables.\n",
    "* Each point on the graph represents a different target points.\n",
    "* The fitted line represents the predicted value.\n",
    "* There are several ways to plot a `regression plot` a simple way to use `Regplot` from the `seaborn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Plot\n",
    "def plot_regression(x, y):\n",
    "    sns.regplot(x=x, y=y, data=df)\n",
    "    plt.ylim(0,)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into independent and dependent variables\n",
    "x = df.drop('price', axis=1) # Independent variables\n",
    "y = df['price'] # Dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in x:\n",
    "    plot_regression(x[col], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above graphs we can say that the some of the data columns are showing us a linear relationship with the target variable and some are not.\n",
    "- So we have to select the columns which are showing linear relationship with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plot\n",
    "- A `residual plot` is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis.\n",
    "- If the points in a `residual plot` are randomly spread out around the x-axis, then a `linear model` is appropriate for the data.\n",
    "- Randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Plot\n",
    "def plot_residual(x, y):\n",
    "    sns.residplot(x=x, y=y, data=df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in x:\n",
    "    plot_residual(x[col], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can say that the `wheel-base`, '`lenght`, `width`, `curb-weight`, `engine-size`, `bore`, `city-l/100Km`, `horsepower` and `highway-mpg` are the best predictor variables for the `price` target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Plot\n",
    "* A distribution plot counts the actual values versus the predicted value. \n",
    "* These plot are extremely useful for visualizing models with more than one independent variables or features.\n",
    "* We can plot the distribution plot using `distplot()` function from `seaborn` library.\n",
    "* We can plot the distribution plot using `matplotlib` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize the process like this: \n",
    "- We have a set of training points. We use these training point to fit and train the model and get parameter.\n",
    "- We then use these parameter in the model to predict the value of the target variable.\n",
    "- We then compare the predicted value with the actual value to see how accurate our model is.\n",
    "- We then use the test data to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model in Python, we import `Linear Model` from `sklearn` library.\n",
    "- We then create a `linear regression` object.\n",
    "- We define the `predictor` and `target` variables.\n",
    "- We then fit the model using `fit()` method.\n",
    "- We then use `predict()` method to predict the value of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = df[['highway-mpg']]\n",
    "Y = df['price']\n",
    "lm = LinearRegression()\n",
    "lm.fit(X,Y)\n",
    "lm.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that instead of having accuracy of 49% our model has high error rate. So we have to improve or change or model.\n",
    "# Multiple Linear Regression\n",
    "- Multiple Linear Regression is very similar to Simple Linear Regression, but this method is used to explain the relationship between one continuous response (dependent) variable and two or more predictor (independent) variables.\n",
    "- Most of the real-world regression models involve multiple predictors. We can use all of the predictors to create a model that predicts the response variable based on the predictors.\n",
    "- The equation is given by:\n",
    "    * `Y: Response/Target/Dependent Variable`\n",
    "    * `X: Predictor/Independent Variable`\n",
    "    * `Y = b0 + b1X1 + b2X2 + b3X3 + ... + bnXn`\n",
    "    * `b0: Intercept`\n",
    "    * `b1: Slope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['wheel-base', 'length', 'width', 'curb-weight', 'engine-size', 'bore', 'horsepower', 'city-L/100Km', 'highway-mpg']]\n",
    "Y = df['price']\n",
    "lm = LinearRegression()\n",
    "lm.fit(X,Y)\n",
    "lm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the R^2 value\n",
    "lm.score(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression \n",
    "- `Polynomial regression` is a particular case of the general linear regression model or multiple linear regression models.\n",
    "- We get non-linear relationships by squaring or setting higher-order terms of the predictor variables.\n",
    "- There are different orders of `polynomial regression`:\n",
    "    * `Quadratic - 2nd order`\n",
    "    * `Cubic - 3rd order`\n",
    "    * `Higher order`\n",
    "- `Polynomial Regression` is beneficial for describing `curvilinear relationship`.\n",
    "- `Curvilinear Relationship` is what we get by `squaring` or setting `higher-order` terms of the predictor variables in the model transforming the data.\n",
    "- The degree of the regression makes a big difference and can result in a better fit if we pick the right value.\n",
    "- In all cases, the relationship between the variables and the parameter is always linear.\n",
    "- In python we can do this by using the `profit()` function from `numpy` library.\n",
    "- `Numpy` library has a function `polyfit()` that takes in the predictor and target variables along with the degree of the polynomial and returns the parameters of the polynomial function.\n",
    "- We can use the `poly1d()` function from `numpy` library to display the polynomial function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = np.polyfit(df['highway-mpg'], df['price'], 3)\n",
    "p = np.poly1d(f)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the model. The symbolic form of the polynomial regression is:\n",
    "- Yhat = a + b1X^2 + b2X^2 + b3X^3 \n",
    "We can also have multi-polynomial linear regression:\n",
    "- Yhat = b0 + b1X1 + b2X2 + b3X1X2 + b4X1^2 + b5X2^2 + ..... + bnX1^n + bn+1X2^n\n",
    "* `Numpy` `polyfit()` function cannot perform this type of regression.\n",
    "- We can use the `PolynomialFeatures()` function in `sklearn.preprocessing` library to transform the original data into a polynomial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pr = PolynomialFeatures(degree=2, include_bias=False)\n",
    "x_polly = pr.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_polly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline \n",
    "- We can simplify our code by using a pipeline library.\n",
    "- There are many steps to getting a prediction for example, `Normalization`, `Polynomial Features` and `Linear Regression`.\n",
    "- Pipeline sequentially performs a series of transformation. The last step carries out a prediction.\n",
    "- First we import all the modules we need. Then we import the library `Pipeline`.\n",
    "- We create a list of tuples:\n",
    "    - First element in the tuples contains the name of the estimator model.\n",
    "    - Second element contains model constructor. \n",
    "    - We now have a pipeline object. We can train the pipeline by applying the train method to the `Pipeline` object.\n",
    "    - We can also produce a prediction as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "Input = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(degree=2)), ('model', LinearRegression())]\n",
    "\n",
    "pipe = Pipeline(Input)\n",
    "pipe.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe.score(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure for In-Smple Evaluation\n",
    "- A way to numerically determine how good the model fits on dataset.\n",
    "- Two important measures to determine the fit of a model: \n",
    "    * `Mean-Squard Error (MSE)`\n",
    "    * `R-squared`\n",
    "### Mean-Squared Error\n",
    "- To measure the MSE, we find the difference between the `actual valule` y and the `predicted value` yhat then square it.\n",
    "- For example, the actual value is 150, the predicted value is 50. Substracting these points we get 100. SWe then square the numbers.\n",
    "- We then take the Mean or Average if all the errors by adding them all together anf dividing by the number of samples.\n",
    "- To find the MSE in Python, we can import the `mean-square-error()` function from `scikit-learn.metrics`.\n",
    "- The `mean-squared-error()` function gets two imputs: \n",
    "    - The `actual value` of the `target variables`.\n",
    "    - The `predicted value` of the `target variables`.\n",
    "\n",
    "# R-squared\n",
    "- It is also called the `coefficient of determination`.\n",
    "- It is a measure to determine how close the data is to the fitted regression line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the R^2 value\n",
    "lm.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = lm.predict(X)\n",
    "mean_squared_error(Y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:\n",
    "* It shows how spread out the errors are.\n",
    "* It's always non-negative.\n",
    "* Values closer to zero are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the RMSE\n",
    "from math import sqrt\n",
    "sqrt(mean_squared_error(Y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean Absolute Error (MAE) is the mean of the absolute value of the errors:\n",
    "* It shows how close the predictions are to the actual values.\n",
    "* It's always non-negative.\n",
    "* Values closer to zero are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(Y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
